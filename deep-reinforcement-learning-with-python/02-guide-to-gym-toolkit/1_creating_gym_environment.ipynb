{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-creating-gym-environment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBNkkbh35FA0KEial1K1Od",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-reinforcement-learning-with-python/blob/main/02-guide-to-gym-toolkit/1_creating_gym_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MEj9VP2VyjD"
      },
      "source": [
        "## Creating gym environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq1YCOq7V9QS"
      },
      "source": [
        "We learned that the gym provides a variety of environments for training the reinforcement learning agent. To clearly understand how the gym environment is designed, we will start off with the basic gym environment. Going forward, we will understand all other complex gym environments. \n",
        "\n",
        "Let's introduce one of the simplest environments called the frozen lake environment. The frozen lake environment is shown below. As we can observe, in the frozen lake environment, the goal of the agent is to start from the initial state S and reach the goal state G.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env1.png?raw=1'/>\n",
        "\n",
        "In the above environment, the following applies:\n",
        "\n",
        "* S denotes the starting state\n",
        "* F denotes the frozen state\n",
        "* H denotes the hole state\n",
        "* G denotes the goal state\n",
        "\n",
        "So, the agent has to start from the state S and reach the goal state G. But one issue is that if the agent visits the state H, which is just the hole state, then the agent will fall into the hole and die as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env2.png?raw=1'/>\n",
        "\n",
        "So, we need to make sure that the agent starts from S and reaches G without falling into the hole state H as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env3.png?raw=1'/>\n",
        "\n",
        "Each grid box in the above environment is called state, thus we have 16 states (S to G) and we have 4 possible actions which are up, down, left and right. We learned that our goal is to reach the state G from S without visiting H. So, we assign reward as 0 to all the states and + 1 for the goal state G. \n",
        "\n",
        "Thus, we learned how the frozen lake environment works. Now, to train our agent in the frozen lake environment, first, we need to create the environment by coding it from scratch in Python. But luckily we don't have to do that! Since the gym provides the various environment, we can directly import the gym toolkit and create a frozen lake environment using the gym.\n",
        "\n",
        "\n",
        "Now, we will learn how to create our frozen lake environment using the gym. Before running any code, make sure that you activated our virtual environment universe. First, let's import the gym library:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bCfWWujYFH_"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVP2GRnfYK91"
      },
      "source": [
        "Next, we can create a gym environment using the make function.  The make function requires the environment id as a parameter. \n",
        "\n",
        "In the gym, the id of the frozen lake environment is `FrozenLake-v0`. So, we can create our frozen lake environment as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKwRRDi5YHZh"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DvLXixgYX7M"
      },
      "source": [
        "After creating the environment, we can see how our environment looks like using the render function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rByHHDGaYV6h",
        "outputId": "0bb5ae3b-4d6a-47cc-e384-6a08fa31c118"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Ew4xuZYhuY"
      },
      "source": [
        "As we can observe, the frozen lake environment consists of 16 states (S to G) as we learned. The state S is highlighted indicating that it is our current state, that is, agent is in the state S. So whenever we create an environment, an agent will always begin from the initial state, in our case, it is the state S. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uz1lgJcYkmA"
      },
      "source": [
        "## Exploring the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xYUIIAYlDJ"
      },
      "source": [
        "We learned that the reinforcement learning environment can be modeled as the Markov decision process (MDP) and an MDP consists of the following: \n",
        "\n",
        "* __States__ -  A set of states present in the environment \n",
        "* __Actions__ - A set of actions that the agent can perform in each state. \n",
        "* __Transition probability__ - The transition probability is denoted by $P(s'|s,a) $. It implies the probability of moving from a state $s$ to the state $s'$ while performing an action $a$.\n",
        "* __Reward function__ - Reward function is denoted by $R(s,a,s')$. It implies the reward the agent obtains moving from a state $s$ to the state  $s'$ while performing an action $a$.\n",
        "\n",
        "Let's now understand how to obtain all the above information from the frozen lake environment we just created using the gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82GngvA0ZlwB"
      },
      "source": [
        "## States"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DwzzeskZmSy"
      },
      "source": [
        "A state space consists of all of our states. We can obtain the number of states in our environment by just typing `env.observation_space` as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xDg1OfUYbKR",
        "outputId": "3028c4ba-4c95-4e49-e1ef-15e6296b5f48"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2qgzT8ZvXU"
      },
      "source": [
        "It implies that we have 16 discrete states in our state space starting from the state S to G. Note that, in the gym, the states will be encoded as a number, so the state S will be encoded as 0, state F will be encoded as 1 and so on as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env4.png?raw=1'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOErF0vyZ6Ud"
      },
      "source": [
        "## Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkPjTtE8Z7uY"
      },
      "source": [
        "We learned that the action space consists of all the possible actions in the environment. We can obtain the action space by `env.action_space` as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuzAqcNZs6Q",
        "outputId": "a7bddecd-a5d6-4999-8390-845e277d6806"
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bP_N1_paLy6"
      },
      "source": [
        "It implies that we have 4 discrete actions in our action space which are left, down, right, up. Note that, similar to states, actions also will be encoded into numbers as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env5.png?raw=1'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm_5UbTYaRwu"
      },
      "source": [
        "## Transition probability and Reward function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIGg3kpqaYo_"
      },
      "source": [
        "Now, let's look at how to obtain the transition probability and the reward function. We learned that in the stochastic environment, we cannot say that by performing some action $a$, agent will always reach the next state $s'$ exactly because there will be some randomness associated with the stochastic environment and by performing an action $a$ in the state $s$, agent reaches the next state  with some probability.\n",
        "\n",
        "Let's suppose we are in state 2 (F). Now if we perform action 1 (down) in state 2, we can reach the state 6 as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env6.png?raw=1'/>\n",
        "\n",
        "Our frozen lake environment is a stochastic environment. When our environment is stochastic we won't always reach the state 6 by performing action 1(down) in state 2, we also reach other states with some probability. So when we perform an action 1 (down) in the state 2, we reach state 1 with probability 0.33333, we reach state 6 with probability 0.33333 and we reach the state 3 with probability 0.33333 as shown below:\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env7.png?raw=1'/>\n",
        "\n",
        "\n",
        "As we can notice, in the stochastic environment we reach the next states with some probability. Now, let's learn how to obtain this transition probability using the gym environment.  \n",
        "\n",
        "We can obtain the transition probability and the reward function by just typing `env.P[state][action]` So, in order to obtain the transition probability of moving from the state S to the other states by performing an action right, we can type, `env.P[S][right]`. But we cannot just type state S and action right directly since they are encoded into numbers. We learned that state S is encoded as 0 and the action right is encoded as 2, so, in order to obtain the transition probability of state S by performing an action right, we type `env.P[0][2]` as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miUXO480aIu0",
        "outputId": "996f4221-e7e1-4e60-fedf-5996e8a10097"
      },
      "source": [
        "print(env.P[0][2])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfxnUw10dDtU"
      },
      "source": [
        "What does this imply? Our output is in the form of `[(transition probability, next state, reward, Is terminal state?)]` It implies that if we perform an action 2 (right) in state 0 (S) then:\n",
        "\n",
        "* We reach the state 4 (F) with probability 0.33333 and receive 0 reward. \n",
        "* We reach the state 1 (F) with probability 0.33333 and receive 0 reward.\n",
        "* We reach the same state 0 (S) with probability 0.33333 and receive 0 reward.\n",
        "\n",
        "The transition probability is shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env8.png?raw=1'/>\n",
        "\n",
        "Thus, when we type `env.P[state][action]` we get the result in the form of `[(transition probability, next state, reward, Is terminal state?)]`. The last value is the boolean and it implies that whether the next state is a terminal state, since 4, 1 and 0 are not the terminal states it is given as false. \n",
        "\n",
        "The output of `env.P[0][2]` is shown in the below table for more clarity:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env9.png?raw=1'/>\n",
        "\n",
        "Let's understand this with one more example. Let's suppose we are in the state 3 (F) as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env10.png?raw=1'/>\n",
        "\n",
        "Say, we perform action 1 (down) in the state 3(F). Then the transition probability of the state 3(F) by performing action 1(down) can be obtained as shown below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JspJX3udc6qg",
        "outputId": "296f68a6-4336-4f9e-e404-3816256f2714"
      },
      "source": [
        "print(env.P[3][1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS6galqxePJz"
      },
      "source": [
        "As we learned, our output is in the form of `[(transition probability, next state, reward, Is terminal state?)]` It implies that if we perform an action 1 (down) in state 3 (F) then:\n",
        "\n",
        "* We reach the state 2 (F) with probability 0.33333 and receive 0 reward. \n",
        "* We reach the state 7 (H) with probability 0.33333 and receive 0 reward.\n",
        "* We reach the same state 3 (F) with probability 0.33333 and receive 0 reward.\n",
        "\n",
        "\n",
        "The transition probability is shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env11.png?raw=1'/>\n",
        "\n",
        "\n",
        "The output of `env.P[3][1]` is shown in the below table for more clarity:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/frozen-lake-env12.png?raw=1'/>\n",
        "\n",
        "As we can observe, in the second row of our output, we have, `(0.33333, 7, 0.0, True)`,and the last value here is marked as True. It implies that state 7 is a terminal state. That is, if we perform action 1(down) in state 3(F) then we reach the state 7(H) with 0.33333 probability and since 7(H) is a hole, the agent dies if it reaches the state 7(H). Thus 7(H) is a terminal state and so it is marked as True. \n",
        "\n",
        "Thus, we learned how to obtain the state space, action space, transition probability and the reward function using the gym environment. In the next, we will learn how to generate an episode. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvxgvg7jwDy4"
      },
      "source": [
        "## Generating an episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKqPykjPwE3T"
      },
      "source": [
        "We learned that the agent-environment interaction starting from an initial state until the terminal state is called an episode.\n",
        "\n",
        "Before we begin, we initialize the state by resetting our environment; resetting puts our agent back to the initial state. \n",
        "\n",
        "We can reset our environment using the reset() function as shown as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abuJrEWewVum"
      },
      "source": [
        "state = env.reset()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlqQiBpvwdzP"
      },
      "source": [
        "### Action selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0sxyGTpwetv"
      },
      "source": [
        "In order for the agent to interact with the environment, it has to perform some\n",
        "action in the environment. So, first, let's learn how to perform an action in the Gym environment. \n",
        "\n",
        "Let's suppose we are in state 1 (F).\n",
        "\n",
        "Say we need to perform action 1 (down) and move to the new state 4 (F). How can\n",
        "we do that? We can perform an action using the step function. We just need to input our action as a parameter to the step function. \n",
        "\n",
        "So, we can perform action 1 (down) in state 1 (S) using the step function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ce-s4TSwYfQ",
        "outputId": "77d3fb2a-bcbc-47b0-ef9b-ae77697c8422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.step(1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 0.0, False, {'prob': 0.3333333333333333})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t4iGwpEy9Mq"
      },
      "source": [
        "Now, let's render our environment using the render function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzC15k1kwuDe",
        "outputId": "6fa299f8-6fa8-4dc9-e2db-4375a427ee0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2Nhz2uzJPK"
      },
      "source": [
        "Note that whenever we make an action using env.step(), it outputs a tuple\n",
        "containing 4 values. So, when we take action 1 (down) in state 1 (S) using env.\n",
        "step(1), it gives the output as:\n",
        "\n",
        "```\n",
        "(4, 0.0, False, {'prob': 0.33333})\n",
        "```\n",
        "\n",
        "As you might have guessed, it implies that when we perform action 1 (down) in state 1 (S):\n",
        "\n",
        "- We reach the next state 4 (F).\n",
        "- The agent receives the reward 0.0.\n",
        "- Since the next state 4 (F) is not terminal state, it is marked as False.\n",
        "- We reach the next state 4 (F) with a probability of 0.33333.\n",
        "\n",
        "So, we can just store this information as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rwWJmqJxwze"
      },
      "source": [
        "(next_state, reward, done, info) = env.step(1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McCWJDonz1PI",
        "outputId": "4c91d0d9-ed65-4365-f0e2-721cd093bd4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(next_state)  # next_state represents the next state"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA4ohQdYz73t",
        "outputId": "317f77d8-2310-437f-8845-39cd5fe03d96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(reward)  # reward represents the obtained reward"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhwGDAQQ0Afh",
        "outputId": "69e83b33-4c13-4f26-a9fd-bcdf09b5822e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(done)  # done implies whether our episode has ended"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snretitK0JaT",
        "outputId": "05ce2f97-00d5-4a4d-f1f0-8d8eddb3ee0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(info)  # which is used for debugging purposes"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'prob': 0.3333333333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VJKFCps0a6C"
      },
      "source": [
        "We can also sample action from our action space and perform a random action to\n",
        "explore our environment. \n",
        "\n",
        "We can sample an action using the sample function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOfP-iRQ0PQA"
      },
      "source": [
        "random_action = env.action_space.sample()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CVcRiRe0kXK"
      },
      "source": [
        "After we have sampled an action from our action space, then we perform our\n",
        "sampled action using our step function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUmxIscL1Lq_"
      },
      "source": [
        "next_state, reward, done, info = env.step(random_action)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnawIldC1iLq"
      },
      "source": [
        "### Generating an episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmgiCx4k1jGW"
      },
      "source": [
        "Now let's learn how to generate an episode. The episode is the agent environment\n",
        "interaction starting from the initial state to the terminal state. The agent interacts with the environment by performing some action in each state. An episode ends if the agent reaches the terminal state. \n",
        "\n",
        "So, in the Frozen Lake environment, the episode will end if the agent reaches the terminal state, which is either the hole state (H) or goal state (G).\n",
        "\n",
        "Let's understand how to generate an episode with the random policy. We learned\n",
        "that the random policy selects a random action in each state. So, we will generate an episode by taking random actions in each state. So for each time step in the episode, we take a random action in each state and our episode will end if the agent reaches the terminal state.\n",
        "\n",
        "First, let's set the number of time steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEnVpAoc21PR",
        "outputId": "38f8b6eb-43f6-443b-bd40-38e10badc513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state = env.reset()\n",
        "print('Time Step 0 :')\n",
        "env.render()\n",
        "num_timesteps = 20"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDoal3iw2Cj1",
        "outputId": "3c8986a6-c28f-4f69-bf61-a1122e9a7d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for t in range(num_timesteps):  # For each time step\n",
        "  # Randomly select an action by sampling from the action space\n",
        "  random_action = env.action_space.sample()\n",
        "  # Perform the selected action\n",
        "  next_state, reward, done, info = env.step(random_action)\n",
        "  print(\"Time Step {} :\".format(t + 1))\n",
        "\n",
        "  env.render()\n",
        "\n",
        "  # If the next state is the terminal state, then break. This implies that our episode ends\n",
        "  if done:\n",
        "    break"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Step 1 :\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLmf3LII3OW_"
      },
      "source": [
        "Instead of generating one episode, we can also generate a series of episodes by taking some random action in each state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEwVzWwb3DDn",
        "outputId": "350bcc87-77cf-47c1-831e-1bab5a0ed7c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_episodes = 10\n",
        "num_timesteps = 20\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  print(\"Time Step 0 :\")\n",
        "  env.render()\n",
        "\n",
        "  for t in range(num_timesteps):\n",
        "    random_action = env.action_space.sample()\n",
        "    next_state, reward, done, info = env.step(random_action)\n",
        "    print(\"Time Step {} :\".format(t + 1))\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 5 :\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 6 :\n",
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 7 :\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 8 :\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 5 :\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 6 :\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 7 :\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 8 :\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 9 :\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 5 :\n",
            "  (Left)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 6 :\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 7 :\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 8 :\n",
            "  (Right)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 9 :\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Time Step 10 :\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 11 :\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 12 :\n",
            "  (Right)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 13 :\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Down)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 5 :\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 6 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 7 :\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 8 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 9 :\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 10 :\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 11 :\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 12 :\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time Step 13 :\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Time Step 14 :\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Time Step 15 :\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Time Step 16 :\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Time Step 17 :\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Time Step 18 :\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 5 :\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 6 :\n",
            "  (Down)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Left)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 4 :\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 0 :\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 1 :\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 2 :\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time Step 3 :\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5QXowzc5XLN"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbjr2OSn5TJ5"
      },
      "source": [
        "Thus, we can generate an episode by selecting a random action in each state by\n",
        "sampling from the action space. \n",
        "\n",
        "But wait! What is the use of this? Why do we even need to generate an episode?\n",
        "\n",
        "we learned that an agent can find the optimal policy (that is, the correct action in each state) by generating several episodes.But in the preceding\n",
        "example, we just took random actions in each state over all the episodes.\n",
        "\n",
        "How can the agent find the optimal policy? So, in the case of the Frozen Lake environment, how can the agent find the optimal policy that tells the agent to reach state G from state S without visiting the hole states H?\n",
        "\n",
        "**This is where we need a reinforcement learning algorithm. Reinforcement learning is all about finding the optimal policy, that is, the policy that tells us what action to perform in each state.**\n",
        "\n",
        "So far we have understood how the Gym environment works using the basic Frozen\n",
        "Lake environment, but Gym has so many other functionalities and also several\n",
        "interesting environments."
      ]
    }
  ]
}