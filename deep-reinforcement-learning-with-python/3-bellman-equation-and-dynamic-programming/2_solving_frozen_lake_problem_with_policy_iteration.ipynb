{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-solving-frozen-lake-problem-with-policy-iteration.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuPGWcsSB1XhdJcNHl3kZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-reinforcement-learning-with-python/blob/main/3-bellman-equation-and-dynamic-programming/2_solving_frozen_lake_problem_with_policy_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzNcOo9b63V1"
      },
      "source": [
        "##Solving the Frozen Lake Problem with Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYRFIbi64GF"
      },
      "source": [
        "Dynamic programming (DP) is a technique for solving complex problems. In DP,\n",
        "instead of solving a complex problem as a whole, we break the problem into simple sub-problems, then for each sub-problem, we compute and store the solution. If the same subproblem occurs, we don't recompute; instead, we use the already computed solution. Thus, DP helps in drastically minimizing the computation time. It has its applications in a wide variety of fields including computer science, mathematics, bioinformatics, and so on.\n",
        "\n",
        "Now, we will learn about two important methods that use DP to find the optimal\n",
        "policy. The two methods are:\n",
        "\n",
        "- Value iteration\n",
        "- Policy iteration\n",
        "\n",
        "Note that dynamic programming is a model-based method meaning that it will help\n",
        "us to find the optimal policy only when the model dynamics (transition probability) of the environment are known. If we don't have the model dynamics, we cannot apply DP methods.\n",
        "\n",
        "\n",
        "Reference:\n",
        "\n",
        "- https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
        "\n",
        "- https://medium.com/analytics-vidhya/rendering-openai-gym-environments-in-google-colab-9df4e7d6f99f\n",
        "\n",
        "- https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB_EPYRJCL-Q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRkY66kn3jZX"
      },
      "source": [
        "**Render OpenAI Gym Environments in CoLab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owxty_FJ9MH8"
      },
      "source": [
        "It is possible to visualize the game your agent is playing, even on CoLab.  This section provides information on how to generate a video in CoLab that shows you an episode of the game your agent is playing. This video process is based on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
        "\n",
        "Begin by installing **pyvirtualdisplay** and **python-opengl**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pGBkJkP3tss"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wosuAMKV3w3I"
      },
      "source": [
        "Next, we install needed requirements to display an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlFxf3kK3xUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2ca026-3498-49cc-ecbb-aedd6dfdbc0d"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (57.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT8Xpzrr35ka"
      },
      "source": [
        "Next we define functions used to show the video by adding it to the CoLab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHdxX2Xu36Oa"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPD1yElU4bOS"
      },
      "source": [
        "The Gym library allows us to query some of these attributes from environments.  I created the following function to query gym environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIHc9eb99G8e"
      },
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ly4Xqp4hyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9f6a75-46d5-499e-8841-d58c3e783cb6"
      },
      "source": [
        "query_environment(\"FrozenLake-v0\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(16)\n",
            "Max Episode Steps: 100\n",
            "Nondeterministic: False\n",
            "Reward Range: (0, 1)\n",
            "Reward Threshold: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XaeI9Ny5BmO"
      },
      "source": [
        "## Policy iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4mkONu35Cac"
      },
      "source": [
        "In the value iteration method, first, we computed the optimal value function by\n",
        "taking the maximum over the $Q$ function ($Q$ values) iteratively. Once we found the\n",
        "optimal value function, we used it to extract the optimal policy. \n",
        "\n",
        "Whereas in policy iteration we try to compute the optimal value function using the policy iteratively, once we found the optimal value function, we can use it to extract the optimal policy.\n",
        "\n",
        "First, let's learn how to compute the value function using a policy. Say we have a policy $\\pi$ , how can we compute the value function using the policy $\\pi$ ?\n",
        "\n",
        "Here, we can use our Bellman equation. We learned that according to the Bellman equation, we can compute the value function using the policy $\\pi$ as the following shows:\n",
        "\n",
        "$$V^{\\pi}(s) = \\sum_{a} \\pi(a|s) \\space \\sum_{s^\"} P(s^\"|s, a)[R(s, a, s^\") + \\gamma V^{\\pi}(s^\")]$$\n",
        "\n",
        "Let's suppose our policy is a deterministic policy, so we can remove the term\n",
        "$\\sum_{a} \\pi(a|s)$ from the preceding equation since there is no stochasticity in the policy and rewrite our Bellman equation as:\n",
        "\n",
        "$$V^{\\pi}(s) = \\sum_{s^\"} P(s^\"|s, a)[R(s, a, s^\") + \\gamma V^{\\pi}(s^\")]$$\n",
        "\n",
        "Thus using the above equation we can compute the value function using a policy.\n",
        "Our goal is to find the optimal value function because once we have found the\n",
        "optimal value function, we can use it to extract the optimal policy.\n",
        "\n",
        "We will not be given any policy as an input. So, we will initialize the random policy\n",
        "and compute the value function using the random policy. Then we check if the\n",
        "computed value function is optimal or not. It will not be optimal since it is computed based on the random policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRZxqzqFyiJI"
      },
      "source": [
        "###Frozen Lake environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsNUKPb7E80"
      },
      "source": [
        "Let's recap the Frozen Lake environment a bit. In the Frozen Lake environment\n",
        "shown below, the following applies:\n",
        "\n",
        "* S implies the starting state\n",
        "* F implies the frozen states\n",
        "* H implies the hold states\n",
        "* G implies the goal state\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-4.png?raw=1'/>\n",
        "\n",
        "We learned that in the frozen lake environment, our goal is to reach the goal state G from the starting state S without visiting the hole states H. That is, while trying to reach the goal state G from the starting state S if the agent visits the hole state H then it will fall into the hole and die as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-5.png?raw=1'/>\n",
        "\n",
        "So, the goal of the agent is to reach the state G starting from the state S without visiting the hole states H as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-6.png?raw=1'/>\n",
        "\n",
        "How can we achieve this goal? That is, how can we reach the state G from S without visiting H? \n",
        "\n",
        "We learned that the optimal policy tells the agent to perform correct action in\n",
        "each state. So, if we find the optimal policy then we can reach the state G from S visiting the state H. \n",
        "\n",
        "Okay, how to find the optimal policy? \n",
        "\n",
        "We can use the value iteration method we just learned to find the optimal policy.\n",
        "\n",
        "\n",
        "Remember that all our states (S to G) will be encoded from 0 to 16 and all the four actions - left, down, up, right will be encoded from 0 to 3 in the gym toolkit.\n",
        "\n",
        "So, we will learn how to find the optimal policy using the value iteration\n",
        "method so that the agent can reach the state G from S without visiting H.\n",
        "\n",
        "Now, let's create the frozen lake environment using gym:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDd9SSSI62QQ"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2M4mo677SRx"
      },
      "source": [
        "Let's look at the frozen lake environment using the render function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPjoYeV77J2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c0c48f-e204-4555-e7ef-b33019bf9f79"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZUmI_M8CBa"
      },
      "source": [
        "As we can notice, our agent is in the state S and it has to reach the state G without visiting the states H. So, let's learn how to compute the optimal policy using the value iteration method.\n",
        "\n",
        "In the value iteration method, we perform two steps:\n",
        "\n",
        "1. Compute the optimal value function by taking the maximum over the $Q$\n",
        "function, that is $V^*(s) = max_{\\alpha} Q^*(s, a)$\n",
        "2. Extract the optimal policy from the computed optimal value function\n",
        "\n",
        "First, let's learn how to compute the optimal value function, and then we will see how to extract the optimal policy from the computed optimal value function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt8f-Y1o8TQ4"
      },
      "source": [
        "## Computing the value function using the policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlEmbmwk8TtD"
      },
      "source": [
        "This step is exactly the same as how we computed the value function in the value\n",
        "iteration method but with a small difference. Here, we compute the value function using the policy but in the value iteration method, we compute the value function by taking the maximum over Q values. Now, let's learn how to define a function that computes the value function using the given policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g2yEpV_8Nx8"
      },
      "source": [
        "def compute_value_function(policy):\n",
        "\n",
        "  # set the number of iterations\n",
        "  num_iterations = 1000\n",
        "  # set the threshold number for checking the convergence of the value function\n",
        "  threshold = 1e-20\n",
        "  # we also set the discount factor\n",
        "  gamma = 1.0\n",
        "\n",
        "  # now, we will initialize the value table, with the value of all states to zero\n",
        "  value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "  # for every iteration\n",
        "  for i in range(num_iterations):\n",
        "    # update the value table, that is, we learned that on every iteration, we use the updated value\n",
        "    # table (state values) from the previous iteration\n",
        "    updated_value_table = np.copy(value_table)\n",
        "\n",
        "    '''\n",
        "    Now, we compute the value function using the given policy. We learned that a value\n",
        "    function can be computed according to some policy.\n",
        "\n",
        "    Thus, for each state, we select the action according to the policy and then we update\n",
        "    the value of the state using the selected action\n",
        "    '''\n",
        "    for s in range(env.observation_space.n):\n",
        "      # Select the action in the state according to the policy\n",
        "      a = policy[s]\n",
        "      # Compute the value of the state using the selected action\n",
        "      value_table[s] = sum([prob*(r + gamma * updated_value_table[s_]) for prob, s_, r, _ in env.P[s][a]])\n",
        "\n",
        "    '''\n",
        "    after computing the value table, that is, value of all the states, we check whether the\n",
        "    difference between value table obtained in the current iteration and previous iteration is\n",
        "    less than or equal to a threshold value if it is less then we break the loop and return the value table as an accurate value function of the given policy\n",
        "    '''\n",
        "    if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "      break\n",
        "\n",
        "  return value_table"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbStKQmZ86eY"
      },
      "source": [
        "Now, that we have computed the optimal value function by taking the maximum over Q values, let's see how to extract the optimal policy from the optimal value function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M6mQPh9d4P"
      },
      "source": [
        "## Extracting policy from the value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXYX4nV9fDP"
      },
      "source": [
        "This step is exactly the same as how we extracted the policy from the value function in the value iteration method. Thus, similar to what we learned in the value iteration method, we define a function called extract_policy to extract a policy given the value function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5GI3AKq9x5i"
      },
      "source": [
        "def extract_policy(value_table):\n",
        "  \n",
        "  # set the discount factor\n",
        "  gamma = 1.0\n",
        "\n",
        "  # First, we initialize the policy with zeros, that is, first, we set the actions for all the states to be zero\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "  '''\n",
        "  Now, we compute the Q function using the optimal value function obtained from the previous step.\n",
        "  After computing the Q function, we can extract policy by selecting action which has maximum Q value.\n",
        "  Since we are computing the Q function using the optimal value function, the policy extracted from the Q function will be the optimal policy.\n",
        "\n",
        "  As shown below, for each state, we compute the Q values for all the actions in the state and \n",
        "  then we extract policy by selecting the action which has maximum Q value.\n",
        "  '''\n",
        "  # for each state\n",
        "  for s in range(env.observation_space.n):\n",
        "    # compute the Q value of all the actions in the state\n",
        "    Q_values = [sum([prob*(r + gamma * value_table[s_]) for prob, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "    # extract policy by selecting the action which has maximum Q value\n",
        "    policy[s] = np.argmax(np.array(Q_values))\n",
        "\n",
        "  return policy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXGq2mOAmMy"
      },
      "source": [
        "That's it! Now, we will see how to extract the optimal policy in our frozen lake environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx9N7x1JAoW4"
      },
      "source": [
        "##Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIXdUqZApR_"
      },
      "source": [
        "First, let's define a function called policy_iteration which takes the environment as a parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PlRUXu_-j1a"
      },
      "source": [
        "def policy_iteration(env):\n",
        "\n",
        "  # set the number of iterations\n",
        "  num_iterations = 1000\n",
        "\n",
        "  # #we learned that in the policy iteration method, we begin by initializing a random policy.\n",
        "  #so, we will initialize the random policy which selects the action 0 in all the states\n",
        "  policy = np.zeros(env.observation_space.n) \n",
        "\n",
        "  # for every iteration\n",
        "  for i in range(num_iterations):\n",
        "    # compute the value function using the policy\n",
        "    value_function = compute_value_function(policy)\n",
        "    # Extract the new policy from the computed value function\n",
        "    new_policy = extract_policy(value_function)\n",
        "\n",
        "    # If policy and new_policy are the same, then break the loop\n",
        "    if (np.all(policy == new_policy)):\n",
        "      break\n",
        "    \n",
        "    # else, update the current policy to new_policy\n",
        "    policy = new_policy\n",
        "\n",
        "  return policy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Hc6wc0Y896"
      },
      "source": [
        "Now, let's learn how to perform policy iteration and find the optimal policy in the frozen lake environment.\n",
        "\n",
        "So, we just feed the frozen lake environment to our policy_iteration function as shown below and get the optimal policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alYj7QcoCrTK"
      },
      "source": [
        "optimal_policy = policy_iteration(env)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2l31X1A8jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5c5cd4-f7df-440c-c1d4-9898a1784687"
      },
      "source": [
        "print(optimal_policy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY9jfubbBxWV"
      },
      "source": [
        "As we can observe, our optimal policy tells us to perform the correct action in each state. Thus, we learned how to perform the policy iteration method to compute the optimal policy."
      ]
    }
  ]
}