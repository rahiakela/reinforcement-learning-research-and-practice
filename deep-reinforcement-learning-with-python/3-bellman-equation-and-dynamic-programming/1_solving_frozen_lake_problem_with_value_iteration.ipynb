{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-solving-frozen-lake-problem-with-value-iteration.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbHxGPAQ2aKV/zxeukXEtK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-reinforcement-learning-with-python/blob/main/3-bellman-equation-and-dynamic-programming/1_solving_frozen_lake_problem_with_value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzNcOo9b63V1"
      },
      "source": [
        "##Solving the Frozen Lake Problem with Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYRFIbi64GF"
      },
      "source": [
        "Dynamic programming (DP) is a technique for solving complex problems. In DP,\n",
        "instead of solving a complex problem as a whole, we break the problem into simple sub-problems, then for each sub-problem, we compute and store the solution. If the same subproblem occurs, we don't recompute; instead, we use the already computed solution. Thus, DP helps in drastically minimizing the computation time. It has its applications in a wide variety of fields including computer science, mathematics, bioinformatics, and so on.\n",
        "\n",
        "Now, we will learn about two important methods that use DP to find the optimal\n",
        "policy. The two methods are:\n",
        "\n",
        "- Value iteration\n",
        "- Policy iteration\n",
        "\n",
        "Note that dynamic programming is a model-based method meaning that it will help\n",
        "us to find the optimal policy only when the model dynamics (transition probability) of the environment are known. If we don't have the model dynamics, we cannot apply DP methods.\n",
        "\n",
        "\n",
        "Reference:\n",
        "\n",
        "- https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
        "\n",
        "- https://medium.com/analytics-vidhya/rendering-openai-gym-environments-in-google-colab-9df4e7d6f99f\n",
        "\n",
        "- https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB_EPYRJCL-Q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRkY66kn3jZX"
      },
      "source": [
        "**Render OpenAI Gym Environments in CoLab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owxty_FJ9MH8"
      },
      "source": [
        "It is possible to visualize the game your agent is playing, even on CoLab.  This section provides information on how to generate a video in CoLab that shows you an episode of the game your agent is playing. This video process is based on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
        "\n",
        "Begin by installing **pyvirtualdisplay** and **python-opengl**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pGBkJkP3tss"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wosuAMKV3w3I"
      },
      "source": [
        "Next, we install needed requirements to display an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlFxf3kK3xUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1add8dd5-9e6c-4e4c-bd30-2d5cc30eaf55"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (57.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT8Xpzrr35ka"
      },
      "source": [
        "Next we define functions used to show the video by adding it to the CoLab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHdxX2Xu36Oa"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPD1yElU4bOS"
      },
      "source": [
        "The Gym library allows us to query some of these attributes from environments.  I created the following function to query gym environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIHc9eb99G8e"
      },
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ly4Xqp4hyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f50b4e-2c21-4f9f-e20d-308c5fb77c73"
      },
      "source": [
        "query_environment(\"FrozenLake-v0\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(16)\n",
            "Max Episode Steps: 100\n",
            "Nondeterministic: False\n",
            "Reward Range: (0, 1)\n",
            "Reward Threshold: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XaeI9Ny5BmO"
      },
      "source": [
        "## Value iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4mkONu35Cac"
      },
      "source": [
        "In the value iteration method, we try to find the optimal policy. We learned that the optimal policy is the one that tells the agent to perform the correct action in each state. \n",
        "\n",
        "In order to find the optimal policy, first, we compute the optimal value\n",
        "function and once we have the optimal value function, we can use it to derive the optimal policy. \n",
        "\n",
        "Okay, how can we compute the optimal value function? We can use our optimal Bellman equation of the value function. We learned that, according to\n",
        "the Bellman optimality equation, the optimal value function can be computed as:\n",
        "\n",
        "$$V^*(s) = max_{\\alpha} \\sum_{s^\"} P(s^\"|s, a)[R(s, a, s^\") + \\gamma V^*(s^\")]$$\n",
        "\n",
        "In the The relationship between the value and Q functions section, we learned that given the value function, we can derive the Q function:\n",
        "\n",
        "$$Q^*(s, a) = \\sum_{s^\"} P(s^\"|s, a)[R(s, a, s^\") + \\gamma V^*(s^\")]$$\n",
        "\n",
        "Thus, we can compute the optimal value function by just taking the maximum over\n",
        "the optimal Q function. So, in order to compute the value of a state, we compute the Q value for all state-action pairs. Then, we select the maximum Q value as the value of the state.\n",
        "\n",
        "$$V^*(s) = max_{\\alpha} Q^*(s, a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRZxqzqFyiJI"
      },
      "source": [
        "###Frozen Lake environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsNUKPb7E80"
      },
      "source": [
        "Let's recap the Frozen Lake environment a bit. In the Frozen Lake environment\n",
        "shown below, the following applies:\n",
        "\n",
        "* S implies the starting state\n",
        "* F implies the frozen states\n",
        "* H implies the hold states\n",
        "* G implies the goal state\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-4.png?raw=1'/>\n",
        "\n",
        "We learned that in the frozen lake environment, our goal is to reach the goal state G from the starting state S without visiting the hole states H. That is, while trying to reach the goal state G from the starting state S if the agent visits the hole state H then it will fall into the hole and die as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-5.png?raw=1'/>\n",
        "\n",
        "So, the goal of the agent is to reach the state G starting from the state S without visiting the hole states H as shown below:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-with-python/vi-6.png?raw=1'/>\n",
        "\n",
        "How can we achieve this goal? That is, how can we reach the state G from S without visiting H? \n",
        "\n",
        "We learned that the optimal policy tells the agent to perform correct action in\n",
        "each state. So, if we find the optimal policy then we can reach the state G from S visiting the state H. \n",
        "\n",
        "Okay, how to find the optimal policy? \n",
        "\n",
        "We can use the value iteration method we just learned to find the optimal policy.\n",
        "\n",
        "\n",
        "Remember that all our states (S to G) will be encoded from 0 to 16 and all the four actions - left, down, up, right will be encoded from 0 to 3 in the gym toolkit.\n",
        "\n",
        "So, we will learn how to find the optimal policy using the value iteration\n",
        "method so that the agent can reach the state G from S without visiting H.\n",
        "\n",
        "Now, let's create the frozen lake environment using gym:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDd9SSSI62QQ"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2M4mo677SRx"
      },
      "source": [
        "Let's look at the frozen lake environment using the render function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPjoYeV77J2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616edf6c-a434-488c-b49f-f51df39aa8ee"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZUmI_M8CBa"
      },
      "source": [
        "As we can notice, our agent is in the state S and it has to reach the state G without visiting the states H. So, let's learn how to compute the optimal policy using the value iteration method.\n",
        "\n",
        "In the value iteration method, we perform two steps:\n",
        "\n",
        "1. Compute the optimal value function by taking the maximum over the $Q$\n",
        "function, that is $V^*(s) = max_{\\alpha} Q^*(s, a)$\n",
        "2. Extract the optimal policy from the computed optimal value function\n",
        "\n",
        "First, let's learn how to compute the optimal value function, and then we will see how to extract the optimal policy from the computed optimal value function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt8f-Y1o8TQ4"
      },
      "source": [
        "## Computing the optimal value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlEmbmwk8TtD"
      },
      "source": [
        "We will define a function called value_iteration where we compute the optimal value function iteratively by taking maximum over $V^*(s) = max_{\\alpha} Q^*(s, a)$. For better understanding, let's closely look at the every line of the function and then we look at the complete function at the end which gives us more clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g2yEpV_8Nx8"
      },
      "source": [
        "def value_iteration(env):\n",
        "\n",
        "  # set the number of iterations\n",
        "  num_iterations = 1000\n",
        "  # set the threshold number for checking the convergence of the value function\n",
        "  threshold = 1e-20\n",
        "  # we also set the discount factor\n",
        "  gamma = 1.0\n",
        "\n",
        "  # now, we will initialize the value table, with the value of all states to zero\n",
        "  value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "  # for every iteration\n",
        "  for i in range(num_iterations):\n",
        "    # update the value table, that is, we learned that on every iteration, we use the updated value\n",
        "    # table (state values) from the previous iteration\n",
        "    updated_value_table = np.copy(value_table)\n",
        "\n",
        "    '''\n",
        "    now, we compute the value function (state value) by taking the maximum of Q value.\n",
        "\n",
        "    thus, for each state, we compute the Q values of all the actions in the state and then \n",
        "    we update the value of the state as the one which has maximum Q value as shown below:\n",
        "    '''\n",
        "    for s in range(env.observation_space.n):\n",
        "      Q_values = [sum([prob*(r + gamma * updated_value_table[s_]) for prob, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "      value_table[s] = max(Q_values)\n",
        "\n",
        "    '''\n",
        "    after computing the value table, that is, value of all the states, we check whether the\n",
        "    difference between value table obtained in the current iteration and previous iteration is\n",
        "    less than or equal to a threshold value if it is less then we break the loop and return the value table as our optimal value function\n",
        "    '''\n",
        "    if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "      break\n",
        "\n",
        "  return value_table"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbStKQmZ86eY"
      },
      "source": [
        "Now, that we have computed the optimal value function by taking the maximum over Q values, let's see how to extract the optimal policy from the optimal value function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M6mQPh9d4P"
      },
      "source": [
        "## Extracting optimal policy from the optimal value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXYX4nV9fDP"
      },
      "source": [
        "In the previous step, we computed the optimal value function. Now, let see how to extract the optimal policy from the computed optimal value function.\n",
        "\n",
        "First, we define a function called extract_policy which takes the value_table as a parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5GI3AKq9x5i"
      },
      "source": [
        "def extract_policy(value_table):\n",
        "  \n",
        "  # set the discount factor\n",
        "  gamma = 1.0\n",
        "\n",
        "  # First, we initialize the policy with zeros, that is, first, we set the actions for all the states to be zero\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "  '''\n",
        "  Now, we compute the Q function using the optimal value function obtained from the previous step.\n",
        "  After computing the Q function, we can extract policy by selecting action which has maximum Q value.\n",
        "  Since we are computing the Q function using the optimal value function, the policy extracted from the Q function will be the optimal policy.\n",
        "\n",
        "  As shown below, for each state, we compute the Q values for all the actions in the state and \n",
        "  then we extract policy by selecting the action which has maximum Q value.\n",
        "  '''\n",
        "  # for each state\n",
        "  for s in range(env.observation_space.n):\n",
        "    # compute the Q value of all the actions in the state\n",
        "    Q_values = [sum([prob*(r + gamma * value_table[s_]) for prob, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "    # extract policy by selecting the action which has maximum Q value\n",
        "    policy[s] = np.argmax(np.array(Q_values))\n",
        "\n",
        "  return policy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXGq2mOAmMy"
      },
      "source": [
        "That's it! Now, we will see how to extract the optimal policy in our frozen lake environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx9N7x1JAoW4"
      },
      "source": [
        "##Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIXdUqZApR_"
      },
      "source": [
        "We learn that in the frozen lake environment our goal is to find the optimal policy which selects the correct action in each state so that we can reach the state G from the state A without visiting the hole states.\n",
        "\n",
        "First, we compute the optimal value function using our value_iteration function by passing our frozen lake environment as the parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PlRUXu_-j1a"
      },
      "source": [
        "optimal_value_function = value_iteration(env=env)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alYj7QcoCrTK",
        "outputId": "5ce97274-befc-43cb-baa1-49c87c002036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(optimal_value_function[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8235294117448217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jd5hruXBas6"
      },
      "source": [
        "Next, we extract the optimal policy from the optimal value function using our extract_policy function as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCw1Hl3NAmUX"
      },
      "source": [
        "optimal_policy = extract_policy(optimal_value_function)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2l31X1A8jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffb2789-76df-4a6b-8023-b922cdb9263f"
      },
      "source": [
        "print(optimal_policy)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY9jfubbBxWV"
      },
      "source": [
        "As we can observe, our optimal policy tells us to perform the correct action in each state.\n",
        "\n",
        "Now, that we have learned what is value iteration and how to perform the value iteration method to compute the optimal policy in our frozen lake environment, in the next section we will learn about another interesting method called the policy iteration."
      ]
    }
  ]
}